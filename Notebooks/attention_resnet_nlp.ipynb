{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Libraries**"
      ],
      "metadata": {
        "id": "pjigsQlR96H_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahm3w4KDe4-Y",
        "outputId": "eb7baffd-974c-4ecf-de02-cb3b4b0aa477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install -q evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libraries**"
      ],
      "metadata": {
        "id": "fPUPkMHh-CZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLMrOzyWeu66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as utils\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from datetime import datetime\n",
        "from tensorboardX import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bk1YnwBe9AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92f45a9-e222-4dd8-d6b2-9931ccaa4d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Attention ResNet**"
      ],
      "metadata": {
        "id": "cv5TxspC-5uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention layers**"
      ],
      "metadata": {
        "id": "xRu7c_-l-GIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN3QRHGDeu7B"
      },
      "outputs": [],
      "source": [
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class SpatialAttn(nn.Module):\n",
        "    def __init__(self, in_features, normalize_attn=True):\n",
        "        super(SpatialAttn, self).__init__()\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, l, g):\n",
        "        N, C, H, W = l.size()\n",
        "        c = self.op(l+g) # (batch_size,1,H,W)\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,H,W)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "        if self.normalize_attn:\n",
        "            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n",
        "        else:\n",
        "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "        return c.view(N,1,H,W), g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Residual block**"
      ],
      "metadata": {
        "id": "azieyOph-RjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZgbUZn2keHg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build the network**"
      ],
      "metadata": {
        "id": "M0wRkrG2-UT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6HtlTNZeu7D"
      },
      "outputs": [],
      "source": [
        "class AttnResNet(nn.Module):\n",
        "    def __init__(self, sample_size, block, layers, num_classes, attention=True, normalize_attn=True):\n",
        "        super(AttnResNet, self).__init__()\n",
        "        # conv blocks\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 512, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(5, stride=2)\n",
        "        self.dense = nn.Conv2d(in_channels=512, out_channels=512, kernel_size= 1, padding=0, bias=True)\n",
        "        # attention blocks\n",
        "        self.attention = attention\n",
        "        if self.attention:\n",
        "            self.projector = ProjectorBlock(128, 512)\n",
        "            self.attn1 = SpatialAttn(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn2 = SpatialAttn(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn3 = SpatialAttn(in_features=512, normalize_attn=normalize_attn)\n",
        "        # final classification layer\n",
        "        if self.attention:\n",
        "            self.classify = nn.Linear(in_features=512*3, out_features=num_classes, bias=True)\n",
        "        else:\n",
        "            self.classify = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer0(x)\n",
        "\n",
        "        l1 = self.layer1(x)\n",
        "\n",
        "        l2 = self.layer2(l1)\n",
        "\n",
        "        l3 = self.layer3(l2)\n",
        "\n",
        "        x = self.avgpool(l3)\n",
        "\n",
        "        #x = x.view(x.size(0), -1)\n",
        "\n",
        "        g = self.dense(x) # batch_sizex512x1x1\n",
        "        # attention\n",
        "        if self.attention:\n",
        "            c1, g1 = self.attn1(self.projector(l1), g)\n",
        "            c2, g2 = self.attn2(l2, g)\n",
        "            c3, g3 = self.attn3(l3, g)\n",
        "            g = torch.cat((g1,g2,g3), dim=1) # batch_sizex3C\n",
        "            # classification layer\n",
        "            x = self.classify(g) # batch_sizexnum_classes\n",
        "        else:\n",
        "            c1, c2, c3 = None, None, None\n",
        "            x = self.classify(torch.squeeze(g))\n",
        "        return [x, c1, c2, c3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train / Val epochs**"
      ],
      "metadata": {
        "id": "lvmYnyzr-X91"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsODuB06eu7G"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, criterion, optimizer, dataloader, device, epoch, log_interval, writer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        # get the inputs and labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        if isinstance(outputs, list):\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        # compute the loss\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # compute the accuracy\n",
        "        prediction = torch.max(outputs, 1)[1]\n",
        "        all_label.extend(labels.squeeze())\n",
        "        all_pred.extend(prediction)\n",
        "        score = accuracy_score(labels.squeeze().cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
        "\n",
        "        # backward & optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            print(\"epoch {:3d} | iteration {:5d} | Loss {:.6f} | Acc {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100))\n",
        "    # Compute the average loss & accuracy\n",
        "    training_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    training_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # Log\n",
        "    writer.add_scalars('Loss', {'train': training_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'train': training_acc}, epoch+1)\n",
        "    print(\"Average Training Loss of Epoch {}: {:.6f} | Acc: {:.2f}%\".format(epoch+1, training_loss, training_acc*100))\n",
        "\n",
        "\n",
        "def val_epoch(model, criterion, dataloader, device, epoch, writer):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            # get the inputs and labels\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # compute the loss\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            losses.append(loss.item())\n",
        "            # collect labels & prediction\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "\n",
        "    # Compute the average loss & accuracy\n",
        "    val_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    val_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # Log\n",
        "    writer.add_scalars('Loss', {'val': val_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'val': val_acc}, epoch+1)\n",
        "    print(\"Average Validation Loss: {:.6f} | Acc: {:.2f}%\".format(val_loss, val_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import dataset**"
      ],
      "metadata": {
        "id": "VzinhcAG-eOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test data directory\n",
        "data_dir = \"/gdrive/MyDrive/data/train_resnet/train/\"\n",
        "test_data_dir = \"/gdrive/MyDrive/data/train_resnet/test/\"\n",
        "\n",
        "#load the train and test data with augmentation\n",
        "dataset = ImageFolder(data_dir,transform = transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "]))\n",
        "test_dataset = ImageFolder(test_data_dir,transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.ToTensor()\n",
        "]))"
      ],
      "metadata": {
        "id": "Dwzr1uS3fZEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2om5ai8eu7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e60944-3b0c-49c3-875c-0f78fbdbd853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Train Data : 5063\n",
            "Length of Validation Data : 500\n"
          ]
        }
      ],
      "source": [
        "batch_size = 7\n",
        "val_size = 500\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
        "print(f\"Length of Train Data : {len(train_data)}\")\n",
        "print(f\"Length of Validation Data : {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebISudMseu7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6777f6d-a44a-41dd-ac7d-3987786065ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "#load the train and validation into batches.\n",
        "train_loader = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
        "val_loader = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size*2, num_workers = 4, pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOKcLWH4eu7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8727a722-fd36-4fc0-cf84-209f59189cd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# switch to cuda\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdjylKHWeu7O"
      },
      "source": [
        "# **Let's train !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9INHofueu7R"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = AttnResNet(sample_size=32, block = ResidualBlock, layers = [3, 4, 6, 3], num_classes=3).to(device)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Using {} GPUs\".format(torch.cuda.device_count()))\n",
        "    model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5zCWBDjeu7S"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(\"runs/cnn_attention_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOA0qA6teu7S"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "lr = 1e-4\n",
        "no_save = False\n",
        "log_interval = 100\n",
        "weight_decay = 1e-4\n",
        "save_path = \"/gdrive/MyDrive/saved_models/resNet/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6arGn2Ueu7T",
        "outputId": "c9509813-d166-4329-e2e7-1cb677e59a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   1 | iteration   100 | Loss 0.771114 | Acc 71.43%\n",
            "epoch   1 | iteration   200 | Loss 0.864530 | Acc 42.86%\n",
            "epoch   1 | iteration   300 | Loss 0.089553 | Acc 100.00%\n",
            "epoch   1 | iteration   400 | Loss 0.107847 | Acc 100.00%\n",
            "epoch   1 | iteration   500 | Loss 0.584193 | Acc 71.43%\n",
            "epoch   1 | iteration   600 | Loss 0.305859 | Acc 71.43%\n",
            "epoch   1 | iteration   700 | Loss 0.055511 | Acc 100.00%\n",
            "Average Training Loss of Epoch 1: 0.526812 | Acc: 79.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.272510 | Acc: 88.60%\n",
            "Saving Model of Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   2 | iteration   100 | Loss 0.120884 | Acc 100.00%\n",
            "epoch   2 | iteration   200 | Loss 0.509735 | Acc 71.43%\n",
            "epoch   2 | iteration   300 | Loss 0.229015 | Acc 85.71%\n",
            "epoch   2 | iteration   400 | Loss 0.240785 | Acc 85.71%\n",
            "epoch   2 | iteration   500 | Loss 0.760544 | Acc 71.43%\n",
            "epoch   2 | iteration   600 | Loss 0.346308 | Acc 85.71%\n",
            "epoch   2 | iteration   700 | Loss 0.010384 | Acc 100.00%\n",
            "Average Training Loss of Epoch 2: 0.260325 | Acc: 89.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.213171 | Acc: 89.80%\n",
            "Saving Model of Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   3 | iteration   100 | Loss 0.035274 | Acc 100.00%\n",
            "epoch   3 | iteration   200 | Loss 0.133436 | Acc 100.00%\n",
            "epoch   3 | iteration   300 | Loss 0.109736 | Acc 100.00%\n",
            "epoch   3 | iteration   400 | Loss 0.085499 | Acc 100.00%\n",
            "epoch   3 | iteration   500 | Loss 0.261006 | Acc 85.71%\n",
            "epoch   3 | iteration   600 | Loss 0.018720 | Acc 100.00%\n",
            "epoch   3 | iteration   700 | Loss 0.078668 | Acc 100.00%\n",
            "Average Training Loss of Epoch 3: 0.198429 | Acc: 92.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.212705 | Acc: 93.80%\n",
            "Saving Model of Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   4 | iteration   100 | Loss 0.025829 | Acc 100.00%\n",
            "epoch   4 | iteration   200 | Loss 0.319995 | Acc 85.71%\n",
            "epoch   4 | iteration   300 | Loss 0.165207 | Acc 85.71%\n",
            "epoch   4 | iteration   400 | Loss 0.154185 | Acc 85.71%\n",
            "epoch   4 | iteration   500 | Loss 0.130651 | Acc 85.71%\n",
            "epoch   4 | iteration   600 | Loss 0.019386 | Acc 100.00%\n",
            "epoch   4 | iteration   700 | Loss 0.018602 | Acc 100.00%\n",
            "Average Training Loss of Epoch 4: 0.172361 | Acc: 93.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.362194 | Acc: 85.20%\n",
            "Saving Model of Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   5 | iteration   100 | Loss 0.014172 | Acc 100.00%\n",
            "epoch   5 | iteration   200 | Loss 0.024898 | Acc 100.00%\n",
            "epoch   5 | iteration   300 | Loss 0.042838 | Acc 100.00%\n",
            "epoch   5 | iteration   400 | Loss 0.057089 | Acc 100.00%\n",
            "epoch   5 | iteration   500 | Loss 0.108253 | Acc 100.00%\n",
            "epoch   5 | iteration   600 | Loss 0.010850 | Acc 100.00%\n",
            "epoch   5 | iteration   700 | Loss 0.015313 | Acc 100.00%\n",
            "Average Training Loss of Epoch 5: 0.143764 | Acc: 94.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.154902 | Acc: 94.20%\n",
            "Saving Model of Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   6 | iteration   100 | Loss 0.009737 | Acc 100.00%\n",
            "epoch   6 | iteration   200 | Loss 0.027866 | Acc 100.00%\n",
            "epoch   6 | iteration   300 | Loss 0.027989 | Acc 100.00%\n",
            "epoch   6 | iteration   400 | Loss 0.022718 | Acc 100.00%\n",
            "epoch   6 | iteration   500 | Loss 0.298004 | Acc 85.71%\n",
            "epoch   6 | iteration   600 | Loss 0.007175 | Acc 100.00%\n",
            "epoch   6 | iteration   700 | Loss 0.001348 | Acc 100.00%\n",
            "Average Training Loss of Epoch 6: 0.120098 | Acc: 95.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.518639 | Acc: 65.80%\n",
            "Saving Model of Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   7 | iteration   100 | Loss 0.052919 | Acc 100.00%\n",
            "epoch   7 | iteration   200 | Loss 0.041425 | Acc 100.00%\n",
            "epoch   7 | iteration   300 | Loss 0.013586 | Acc 100.00%\n",
            "epoch   7 | iteration   400 | Loss 0.705341 | Acc 85.71%\n",
            "epoch   7 | iteration   500 | Loss 0.050600 | Acc 100.00%\n",
            "epoch   7 | iteration   600 | Loss 0.005113 | Acc 100.00%\n",
            "epoch   7 | iteration   700 | Loss 0.078827 | Acc 100.00%\n",
            "Average Training Loss of Epoch 7: 0.127966 | Acc: 95.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.122462 | Acc: 95.80%\n",
            "Saving Model of Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   8 | iteration   100 | Loss 0.009550 | Acc 100.00%\n",
            "epoch   8 | iteration   200 | Loss 0.004395 | Acc 100.00%\n",
            "epoch   8 | iteration   300 | Loss 0.166238 | Acc 85.71%\n",
            "epoch   8 | iteration   400 | Loss 0.022646 | Acc 100.00%\n",
            "epoch   8 | iteration   500 | Loss 0.004125 | Acc 100.00%\n",
            "epoch   8 | iteration   600 | Loss 0.225101 | Acc 85.71%\n",
            "epoch   8 | iteration   700 | Loss 0.338227 | Acc 85.71%\n",
            "Average Training Loss of Epoch 8: 0.097778 | Acc: 96.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.138703 | Acc: 96.20%\n",
            "Saving Model of Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   9 | iteration   100 | Loss 0.008712 | Acc 100.00%\n",
            "epoch   9 | iteration   200 | Loss 0.003710 | Acc 100.00%\n",
            "epoch   9 | iteration   300 | Loss 0.028333 | Acc 100.00%\n",
            "epoch   9 | iteration   400 | Loss 0.003344 | Acc 100.00%\n",
            "epoch   9 | iteration   500 | Loss 0.135930 | Acc 85.71%\n",
            "epoch   9 | iteration   600 | Loss 0.045928 | Acc 100.00%\n",
            "epoch   9 | iteration   700 | Loss 0.118588 | Acc 100.00%\n",
            "Average Training Loss of Epoch 9: 0.098993 | Acc: 96.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.084155 | Acc: 97.40%\n",
            "Saving Model of Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  10 | iteration   100 | Loss 0.465709 | Acc 85.71%\n",
            "epoch  10 | iteration   200 | Loss 0.020143 | Acc 100.00%\n",
            "epoch  10 | iteration   300 | Loss 0.007004 | Acc 100.00%\n",
            "epoch  10 | iteration   400 | Loss 0.000446 | Acc 100.00%\n",
            "epoch  10 | iteration   500 | Loss 0.003053 | Acc 100.00%\n",
            "epoch  10 | iteration   600 | Loss 0.006609 | Acc 100.00%\n",
            "epoch  10 | iteration   700 | Loss 0.011141 | Acc 100.00%\n",
            "Average Training Loss of Epoch 10: 0.076892 | Acc: 97.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.351149 | Acc: 89.60%\n",
            "Saving Model of Epoch 10\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_epoch(model, criterion, optimizer, train_loader, device, epoch, log_interval, writer)\n",
        "    val_epoch(model, criterion, val_loader, device, epoch, writer)\n",
        "    # adjust learning rate\n",
        "    # scheduler.step()\n",
        "    if not no_save:\n",
        "        torch.save(model.state_dict(), os.path.join(save_path, \"cnn_epoch{:03d}.pth\".format(epoch+1)))\n",
        "        print(\"Saving Model of Epoch {}\".format(epoch+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the model**"
      ],
      "metadata": {
        "id": "ABCO4Fxa-lPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAJrvgH5xcI7"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/gdrive/MyDrive/saved_models/resNet/resNet_model_state.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSg4ySb1xoP5",
        "outputId": "f68244d3-303d-4dc7-f02b-62dcd998b03d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "modelB = AttnResNet(sample_size=32, block = ResidualBlock, layers = [3, 4, 6, 3], num_classes=3).to(device)\n",
        "modelB.load_state_dict(torch.load(\"/gdrive/MyDrive/saved_models/resNet/resNet_model_state.pth\"), strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "7T2rXaB5-ofZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 10\n",
        "lr = 1e-4\n",
        "no_save = False\n",
        "log_interval = 100\n",
        "weight_decay = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "writer = SummaryWriter(\"runs/cnn_attention_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now()))"
      ],
      "metadata": {
        "id": "bYlI7_g37iWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfXyw3XNofFp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, criterion, dataloader, device,  writer):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            # get the inputs and labels\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # compute the loss\n",
        "            try :\n",
        "\n",
        "              loss = criterion(outputs, labels.squeeze())\n",
        "              losses.append(loss.item())\n",
        "              # collect labels & prediction\n",
        "              prediction = torch.max(outputs, 1)[1]\n",
        "              all_label.extend(labels.squeeze())\n",
        "              all_pred.extend(prediction)\n",
        "            except :\n",
        "              pass\n",
        "\n",
        "    # Compute the average loss & accuracy\n",
        "    val_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    #val_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # Log\n",
        "    #writer.add_scalars('Loss', {'val': val_loss})\n",
        "    #writer.add_scalars('Accuracy', {'val': val_acc})\n",
        "    #print(\"Average Validation Loss: {:.6f} | Acc: {:.2f}%\".format(val_loss, val_acc*100))\n",
        "\n",
        "    # Classification report\n",
        "    print('\\n\\n\\t\\tCLASSIFICATIION METRICS\\n')\n",
        "    print(metrics.classification_report(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy(),\n",
        "                                        target_names = [\"handwiriting\", \"other\", \"table\"]))\n",
        "    return metrics.classification_report(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy(),\n",
        "                                        target_names = [\"handwiriting\", \"other\", \"table\"], output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBJsQLLYZXOf",
        "outputId": "cd5e258b-f330-43b3-d70b-c25627690cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\t\tCLASSIFICATIION METRICS\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "handwiriting       1.00      0.95      0.97       424\n",
            "       other       0.80      1.00      0.89       465\n",
            "       table       0.99      0.81      0.89       502\n",
            "\n",
            "    accuracy                           0.91      1391\n",
            "   macro avg       0.93      0.92      0.92      1391\n",
            "weighted avg       0.93      0.91      0.91      1391\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = evaluate_model(model, criterion, test_loader, device, writer)\n",
        "df = pd.DataFrame(report).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\t\\tCLASSIFICATIION METRICS\\n\\tCNN with attention mechanism for pipeline ')\n",
        "print('______________________________________________________')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "td3VjfGf4QuZ",
        "outputId": "b340fe93-9684-49c9-e74e-457cd7d1cb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tCLASSIFICATIION METRICS\n",
            "\tCNN with attention mechanism for pipeline \n",
            "______________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              precision    recall  f1-score      support\n",
              "handwiriting   1.000000  0.990566  0.995261   424.000000\n",
              "other          0.965293  0.956989  0.961123   465.000000\n",
              "table          0.954813  0.970060  0.962376   501.000000\n",
              "accuracy       0.971942  0.971942  0.971942     0.971942\n",
              "macro avg      0.973369  0.972538  0.972920  1390.000000\n",
              "weighted avg   0.972103  0.971942  0.971988  1390.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a28614e1-bf25-4e0f-9abd-e2cec070acc6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>handwiriting</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990566</td>\n",
              "      <td>0.995261</td>\n",
              "      <td>424.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other</th>\n",
              "      <td>0.965293</td>\n",
              "      <td>0.956989</td>\n",
              "      <td>0.961123</td>\n",
              "      <td>465.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>table</th>\n",
              "      <td>0.954813</td>\n",
              "      <td>0.970060</td>\n",
              "      <td>0.962376</td>\n",
              "      <td>501.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.971942</td>\n",
              "      <td>0.971942</td>\n",
              "      <td>0.971942</td>\n",
              "      <td>0.971942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.973369</td>\n",
              "      <td>0.972538</td>\n",
              "      <td>0.972920</td>\n",
              "      <td>1390.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.972103</td>\n",
              "      <td>0.971942</td>\n",
              "      <td>0.971988</td>\n",
              "      <td>1390.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a28614e1-bf25-4e0f-9abd-e2cec070acc6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a28614e1-bf25-4e0f-9abd-e2cec070acc6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a28614e1-bf25-4e0f-9abd-e2cec070acc6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "-tKZa6Mg-rzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geQ01at8opD2"
      },
      "outputs": [],
      "source": [
        "def to_device(data, device):\n",
        "    \"Move data to the device\"\n",
        "    if isinstance(data,(list,tuple)):\n",
        "        return [to_device(x,device) for x in data]\n",
        "    return data.to(device,non_blocking = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a5uu5choNvz"
      },
      "outputs": [],
      "source": [
        "def predict_img_class(img,model):\n",
        "    \"\"\" Predict the class of image and Return Predicted Class\"\"\"\n",
        "    img = to_device(img.unsqueeze(0), device)\n",
        "    outputs =  model(img)[0]\n",
        "    prediction = torch.max(outputs, 1)[1]\n",
        "    \"\"\"print(prediction)\n",
        "    _, preds = torch.max(prediction, dim = 1)\"\"\"\n",
        "    return dataset.classes[prediction[0].item()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "LgnPkTp-oPs0",
        "outputId": "600433bb-e27d-4ca1-f5f4-554a0b767490"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-ee73da4ef97c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#open image file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/MyDrive/test_images/handwriting6.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#resize image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/MyDrive/test_images/handwriting6.jpg'"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "#open image file\n",
        "img = Image.open(\"/gdrive/MyDrive/test_images/handwriting5.jpg\")\n",
        "\n",
        "#resize image\n",
        "img = img.resize((150,150))\n",
        "\n",
        "channels = transforms.ToTensor()(img)\n",
        "if channels.shape[0]>3 or channels.shape[0]<3:\n",
        "  img = img.convert(mode='RGB')\n",
        "\n",
        "#convert image to tensor\n",
        "img = transforms.ToTensor()(img)\n",
        "\n",
        "#print image\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "\n",
        "#prdict image label\n",
        "print(f\"Predicted Class : {predict_img_class(img,modelB)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}